% !TeX encoding = UTF-8
% !TeX program = pdflatex
% !TeX spellcheck = it_IT

\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{mathtools}

 
\title{k-WTA Activation Function}
\author{Marco Costa, Andrea Luca Antonini}

\begin{document}

\maketitle

\section{Introduction}
The purpose of this project is to implement a Neural Network in which we use k-WTA as activation function. \newline
The use of k-WTA acivation is motivated for defending against gradient-based adversarial attacks. In fact, provided a labeled data item (\textbf{x},y), the attacker finds a perturbation \textbf{x'} imperceptibly similar to \textbf{x} but misleading enough to cause the network to output a label different from y. The most effective way to find such a perturbation, i.e. adversarial example, is by exploiting the gradient information of the network w.r.t. its input \textbf{x}. \newline
k-WTA activation function takes as input the entire output of a layer, retains its \textit{k} largest values and deactivates all others to zero. If we use $f(\textbf{x};\textbf{w})$ to denote a k-WTA network taking an input \textbf{x} and parameterized by weights \textbf{w}, then the gradient $\nabla_{\textbf{x}}f(\textbf{x};\textbf{w})$ at certain \textbf{x} is undefined. Therefore, $f(\textbf{x};\textbf{w})$ is C$^0$ discontinous. The discontinuities in the activation function also renders $f(\textbf{x};\textbf{w})$ discontinous w.r.t. \textbf{w}, but these discontinuities are rather sparse in the space of \textbf{w}, thus the network can be trained succesfully. \newline
Formally, k-WTA retains the \textit{k} largest values of a $Nx1$ input vector and sets all others to be zero before feeding the vector to the next network layer:
\begin{equation}
\phi_{k}(\textbf{y})_{j} = 
\begin{cases*}
y_{j}$ , if $y_{j} \in\{$ k largest elements of \textbf{y} $\} \\
0$ , otherwise$
\end{cases*}
\end{equation}
where $\phi_{k}: \mathbb{}{R}^{N} \rightarrow \mathbb{R}^{N}$ is the k-WTA function (parameterized by an integer \textit{k}), \textbf{y} $\in \mathbb{R}^{N}$ is the input to the activation, and $\phi_{k}(\textbf{y})_{j}$ denotes the j-th element of the output $\phi_{k}(\textbf{y})$.\\
Instead of specifying \textit{k}, we introduce a parameter $\gamma \in \{0,1\}$ called \emph{sparsity ratio}. If a layer has an ouput dimension $N$, then its k-WTA activation has  $k = \lfloor\gamma \cdot N\rfloor$. Even though the sparsity ratio can be set differently for different layers, in practice there is no gain from introducing such a variation, so we use a fixed $\gamma$.\\
In a Convolutional Neural Network (CNN), the output of a layer is a C x H x W tensor: in this case we will treat the tensor as a C $\cdot$ H $\cdot$ W x 1 vector input to the k-WTA activation function.\\
The runtime of computing a k-WTA activation is asymptotically $O(N)$, because finding \textit{k} largest value in a list corresponds to finding the k-th largest value, which has $O(N)$ complexity.\\
Concerning the training of k-WTA networks, we know that when the sparsity ratio $\gamma$ is relatively small ($\le 0.2$), the network training converges slowly. In fact, a smaller $\gamma$ activates fewer neurons, effectively reducing more of the layer width and, therefore, the network size. Nevertheless, we prefer a smaller $\gamma$ because it usually leads to better robustness against finding adversarial examples.\\
Theoretically speaking, consider one layer outputting values \textbf{x}, passed through  a k-WTA activation, and followed by the next layer whose linear weight matrix is
W. We define the k-WTA \emph{activation pattern} under the input \textbf{x} as:
\begin{center}
$\mathcal{A}(\textbf{x}) := \{i \in [l]$ | $x_{i}$ is one of the \textit{k} largest values in \textbf{x}$\} \subseteq [l]$
\end{center}
where we use $[l]$ to indicate integer set $\{1,...,l\}$.\\
Even an infinitesimal perturbation of \textbf{x} can change $\mathcal{A}(\textbf{x})$: some element $i$ is removed from $\mathcal{A}(\textbf{x})$, while another element $j$ is added in. Corresponding to this change, in the evaluation of $W\phi_{k}(\textbf{x})$, the contribution of $W$'s column vector $W_{i}$ is useless, while another column vector $W_{j}$ suddenly takes effect. It's this change that makes the result of $W\phi_{k}(\textbf{x})$ $C^0$ discontinous.\\
Once $W$ is determined, the discontinuity jump depends on the value of $x_i$ and $x_j$, that are respectively the input value that does no more affect the result and the new value that actually has effect. We note from previous experiments that the smaller $\gamma$ is, the larger the discontinuity jump will be: for this reason, we prefer a smaller $\gamma$, that will make the search of adversarial examples harder.\\
If the activation patterns of all layers are fixed, then f(\textbf{x}) is a linear function, but when the activation pattern changes, f(\textbf{x}) switches from one linear function to another linear function. Over the entire space of \textbf{x}, f(\textbf{x}) is \emph{piecewise linear}. The specific activation patterns of all layers define a specific linear piece of the function, i.e. a \emph{linear region}: in k-WTA, these linear regions are disconnected.\\
If the linear regions are densely distributed, a small perturbation $\Delta\textbf{x}$ from any data point will likely cross the boundary of the linear region where \textbf{x} is. Whenever boundary crossing occurs, the gradient becomes undefined.\\
The rationale behind k-WTA activation is to destroy network gradients, that are information needed in \emph{white box attacks}.\\
k-WTA is able to universally improve the white-box robustness, regardless of the training method. The k-WTA robustness under \emph{black box attacks} is not always significantly better than other activation function networks (e.g. ReLU).
 

	

\end{document}